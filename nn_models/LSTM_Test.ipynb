{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Test",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDLWmZk-ko6y"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "from time import sleep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7xMO9OEkr5o"
      },
      "source": [
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvA0NE26ktbu"
      },
      "source": [
        "# Model Hyperparams \n",
        "input_size = 28 \n",
        "sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_classes = 10\n",
        "\n",
        "# Training Hyperparams\n",
        "num_epochs = 3\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create Data Loader \n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YCSmoDUpv7p"
      },
      "source": [
        "Custom LSTM-module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzVNvmqxk0Ei"
      },
      "source": [
        "class emiLSTM_ver1(nn.Module):  # TODO: multiple LSTM:s on top of each other (num_layers)?, directions?\n",
        "  \"\"\" Own implementation of LSTM.\n",
        "  A single layer LSTM progressing in one single direction. The implemented equations for \n",
        "  updating cell and hidden state can be found at https://colah.github.io/posts/2015-08-Understanding-LSTMs/. \"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    \"\"\" Sets the size of the input at each timestep (input_size) and the size \n",
        "    of the hidden vector (hidden_size) and initializes weights. Note that the size of the cell state\n",
        "    is the same hidden_size per definition. \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "    self.input_size = input_size  # Size of the input-values in sequence\n",
        "    self.hidden_size = hidden_size  # Note that the size of the cellstate is equal to the hidden_size in the standard architecture\n",
        "\n",
        "    # Forget-gate layer parameters\n",
        "    self.Wf = nn.Parameter(torch.zeros(hidden_size, hidden_size + input_size)) # dims pÃ¥ denna\n",
        "    self.bf = nn.Parameter(torch.zeros(hidden_size, 1))\n",
        "\n",
        "    # Input-gate layer parameters\n",
        "    self.Wi = nn.Parameter(torch.zeros(hidden_size, hidden_size + input_size))\n",
        "    self.bi = nn.Parameter(torch.zeros(hidden_size, 1))\n",
        "\n",
        "    # Candidate parameters\n",
        "    self.Wc = nn.Parameter(torch.zeros(hidden_size, hidden_size + input_size))\n",
        "    self.bc = nn.Parameter(torch.zeros(hidden_size, 1))\n",
        "\n",
        "    # Output-gate layer parameters\n",
        "    self.Wo = nn.Parameter(torch.zeros(hidden_size, hidden_size + input_size))\n",
        "    self.bo = nn.Parameter(torch.zeros(hidden_size, 1))\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):   # TODO: CHANGE THIS?\n",
        "    \"\"\" Sets the weights in a standard way. \"\"\"\n",
        "\n",
        "    stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "    for weight in self.parameters():\n",
        "        weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "  def forward(self, X, H_t=None, S_t=None): \n",
        "    \"\"\" Makes the forwardpass over each sequence in a batch training data (X) simultenously. \n",
        "    X is a batch of training data and needs to be on the form: [seq_length (image row direction), input_size (image column direction), batch_size] \"\"\"\n",
        "\n",
        "    batch_size, _, input_size = X.shape\n",
        "    X = torch.transpose(X, 0, 2)\n",
        "    X = torch.transpose(X, 0, 1)\n",
        "\n",
        "    if H_t is None:\n",
        "      H_t = torch.zeros(self.hidden_size, batch_size)\n",
        "    if S_t is None:\n",
        "      S_t = torch.zeros(self.hidden_size, batch_size)\n",
        "\n",
        "\n",
        "    # The forward pass for each sequence in batch\n",
        "    hidden_sequence = []\n",
        "    for t in range(sequence_length):\n",
        "      X_t = X[t, :, :]  # extracts the input vector at timestep t for each sequence in the batch, dim: [input_size, batch_size]. For an image the t:th row is the input vector.\n",
        "      X_and_H = torch.cat((X_t, H_t), 0)\n",
        "\n",
        "      # Update cell state (S_t)\n",
        "      F_t = torch.sigmoid(self.Wf @ X_and_H + self.bf)\n",
        "      I_t = torch.sigmoid(self.Wi @ X_and_H + self.bi)\n",
        "      C_t = torch.tanh(self.Wc @ X_and_H + self.bc)  \n",
        "      S_t = F_t * S_t + I_t * C_t  # Hadamard product\n",
        "\n",
        "      # Update hidden state (H_t)\n",
        "      O_t = torch.sigmoid(self.Wo @ X_and_H + self.bo)\n",
        "      H_t = O_t * torch.tanh(S_t)\n",
        "\n",
        "      hidden_sequence.append(H_t.unsqueeze(0)) # unsqueeze necessary for concatenation at the end\n",
        "\n",
        "    # Concatenate and reshape\n",
        "    hidden_sequences = torch.cat(hidden_sequence)\n",
        "    hidden_sequences = torch.transpose(hidden_sequences, 1, 2)\n",
        "    hidden_sequences = torch.transpose(hidden_sequences, 0, 1)\n",
        "\n",
        "    # A regular LSTM would have the return statement below...    \n",
        "    return hidden_sequences, (H_t, S_t) # hidden_sequences format: [seq_length, batch_size, hidden_size]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OEQYsvlEVb"
      },
      "source": [
        "class LSTM_based_RNN(nn.Module):\n",
        "  \"\"\" RNN that uses an LSTM module and an extra linear output-layer \"\"\"\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super().__init__()\n",
        "    self.num_layers = 1  # this only works for pytorch's implementation\n",
        "    self.hidden_size = hidden_size\n",
        "          \n",
        "    # CHANGE THIS TO COMPARE LSTM:s\n",
        "    #self.lstm = nn.LSTM(input_size, hidden_size, num_layers = self.num_layers, batch_first=True)\n",
        "    self.lstm = emiLSTM_ver1(input_size, hidden_size)\n",
        "\n",
        "    self.end_layer = nn.Linear(hidden_size, num_classes)\n",
        "      \n",
        "  def forward(self, X):  # -> x needs to be: (batch_size, seq_length, input_size)      \n",
        "\n",
        "    if type(self.lstm).__name__ == 'emiLSTM_ver1': \n",
        "      out, _ = self.lstm(X)\n",
        "    else:\n",
        "      # Pytorch LSTM module requires setting initial  hidden and cell states outside module\n",
        "      h0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(device) \n",
        "      c0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(device) \n",
        "      out, _ = self.lstm(X, (h0,c0))  \n",
        "  \n",
        "    # out: [batch_size, seq_length, hidden_size] in both cases since we're using: batch_first=True\n",
        "\n",
        "    out = out[:, -1, :]  # Hidden states for each sample at last time step. Dims: [batch_size, hidden_size]\n",
        "    out = self.end_layer(out)\n",
        "    return out  # dims: [N, 10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WuMKGdqpsgw"
      },
      "source": [
        "Model Training / Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67YaFByLpq7b",
        "outputId": "66d2c04c-106b-4159-bc74-fba9d36f45fd"
      },
      "source": [
        "model = LSTM_based_RNN(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "\n",
        "        # Prepare batch_data\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)  #  from [batch_size, channels, seq_length, input_size] to [batch_size, seq_length, input_size]\n",
        "\n",
        "        # Forward-pass\n",
        "        outputs = model(images) # images needs to be: [batch_size, seq_length (image rows), input_size (image columns)]\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward-pass and grad descent\n",
        "        optimizer.zero_grad()  # set gradients to zero, otherwise they accumulate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}/{num_epochs} | Batch nr. {i+1}/{n_total_steps} | Loss: {loss.item():.4f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/3 | Batch nr. 100/600 | Loss: 0.9142\n",
            "Epoch: 1/3 | Batch nr. 200/600 | Loss: 0.4555\n",
            "Epoch: 1/3 | Batch nr. 300/600 | Loss: 0.3580\n",
            "Epoch: 1/3 | Batch nr. 400/600 | Loss: 0.2649\n",
            "Epoch: 1/3 | Batch nr. 500/600 | Loss: 0.2165\n",
            "Epoch: 1/3 | Batch nr. 600/600 | Loss: 0.2388\n",
            "Epoch: 2/3 | Batch nr. 100/600 | Loss: 0.2751\n",
            "Epoch: 2/3 | Batch nr. 200/600 | Loss: 0.2318\n",
            "Epoch: 2/3 | Batch nr. 300/600 | Loss: 0.1760\n",
            "Epoch: 2/3 | Batch nr. 400/600 | Loss: 0.1790\n",
            "Epoch: 2/3 | Batch nr. 500/600 | Loss: 0.0756\n",
            "Epoch: 2/3 | Batch nr. 600/600 | Loss: 0.0982\n",
            "Epoch: 3/3 | Batch nr. 100/600 | Loss: 0.1136\n",
            "Epoch: 3/3 | Batch nr. 200/600 | Loss: 0.1073\n",
            "Epoch: 3/3 | Batch nr. 300/600 | Loss: 0.1303\n",
            "Epoch: 3/3 | Batch nr. 400/600 | Loss: 0.1154\n",
            "Epoch: 3/3 | Batch nr. 500/600 | Loss: 0.0582\n",
            "Epoch: 3/3 | Batch nr. 600/600 | Loss: 0.0401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2viJESVBDNF"
      },
      "source": [
        "Accuracy on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLQvkDiVqYXd",
        "outputId": "a9702c02-b3ad-4e42-d4dd-ca4b5461b1cd"
      },
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of network on test set (10000 images): {acc} %')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of network on test set (10000 images): 97.32 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}